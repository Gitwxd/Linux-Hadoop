配置好core-site.xml、mapred-site.xml、yarn-site.xml
------------------------------------------------------core-site.xml------------------------------------------------------------------
<configuration>
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://mini1:9000</value>
</property>

<property>
  <name>hadoop.tmp.dir</name>
  <value>/home/hadoop/hadoop/hdpdata</value>
</property>
</configuration>
---------------------------------------------------------mapred-site.xml---------------------------------------------------------
<configuration>
    <property>
	<name>mapreduce.framework.name</name>
	<value>yarn</value>
    </property>
</configuration>
---------------------------------------------------------yarn-site.xml-------------------------------------------------------------
<configuration>

<!-- Site specific YARN configuration properties -->
        <property>
		<name>yarn.resourcemanager.hostname</name>
		<value>mini1</value>
	</property>
		<!-- reducer获取数据的方式 -->
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
</configuration>
运行时会报错，原因是linux上不识别本地的指令，需要对org.apache.hadoop.mapred里的YARNRunner进行改造，改造方法如下：
// Setup the command to run the AM
		List<String> vargs = new ArrayList<String>(8);
		// vargs.add(MRApps.crossPlatformifyMREnv(jobConf,
		// Environment.JAVA_HOME)
		// + "/bin/java");
		// TODO   ----
		System.out.println(MRApps.crossPlatformifyMREnv(jobConf, Environment.JAVA_HOME) + "/bin/java");
		System.out.println("$JAVA_HOME/bin/java");
		vargs.add("$JAVA_HOME/bin/java");
    
    
    //获取环境变量并遍历
    for (String key : environment.keySet()) {
			String org = environment.get(key);
			String linux = getLinux(org);
			environment.put(key, linux);
		}
    
    // 新写方法getLinux 将环境变量进行修改
    private String getLinux(String org) {
		StringBuilder sb = new StringBuilder();
		int c = 0;
		for (int i = 0; i < org.length(); i++) {
			if (org.charAt(i) == '%') {
				c++;
				if (c % 2 == 1) {
					sb.append("$");
				}
			} else {
				switch (org.charAt(i)) {
				case ';':
					sb.append(":");
					break;

				case '\\':
					sb.append("/");
					break;
				default:
					sb.append(org.charAt(i));
					break;
				}
			}
		}
		return (sb.toString());
	}

    
-----------------------------------------------------------Driver--------------------------------------------------------
package top.dbwxd.bigdata.mr.wordcount.demo;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

/**
 * 相当于yarn的集群的客户端
 * @author Jsckson W
 *
 */
public class WordcountDriver {
	public static void main(String[] args) throws Exception {
		
		Configuration conf = new Configuration();
		
		//本地的情况下 可以不需要配置
		//conf.set("mapreduce.framework.name", "yarn");
		//conf.set("yarn.resourcemanager", "mini1");
		//conf.set("mapreduce.framework.name", "local");
		//conf.set("fs.defauleFS", "file:///");
		//conf.set("fs.defauleFS", "hdfs:mini1:9000");
		Job job = Job.getInstance(conf);
		
		//指定程序所在的jar包路径
		job.setJar("F:/ExportJar/wordcount.jar");
		job.setJarByClass(WordcountDriver.class);
		//指定类
		job.setMapperClass(WordcountMapper.class);
		job.setReducerClass(WordcountReducer.class);
		//指定	mapper输出类型
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		
		//指定 reducer的输出类型
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		//指定原数据
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		//
		//job.submit();
		boolean res = job.waitForCompletion(true);
		System.exit(res?0:1);
		
		
	}
}

---------------------------------------------------------------NOTICE-----------------------------------------------------
RUN ARGUMENTS需要相关的参数
--输入路径
--输出路径
--DHADOOP_USER_NAME

